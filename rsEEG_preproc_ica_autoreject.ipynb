{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the relevant stuff\n",
    "\n",
    "# eeg analysis libraries\n",
    "import mne\n",
    "\n",
    "# import plotting libraries\n",
    "from matplotlib import pyplot as plt\n",
    "# python version of ggplot:\n",
    "from plotnine import *\n",
    "# https://github.com/sbebo/joypy/blob/master/Joyplot.ipynb\n",
    "import joypy\n",
    "import seaborn as sns\n",
    "#https://github.com/pog87/PtitPrince/blob/master/RainCloud_Plot.ipynb\n",
    "import PtitPrince  as pt\n",
    "\n",
    "# numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# import tqdm for a progress bar:\n",
    "try:\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "except:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# import file operators\n",
    "from pathlib import Path\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "# avoid being too verbose\n",
    "import warnings\n",
    "mne.set_log_level('ERROR')\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "#import autoreject\n",
    "from autoreject import AutoReject\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and events\n",
    "from data import raws, events, n\n",
    "for raw in raws():\n",
    "    print(raw.info['subject_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run analysis across all participants\n",
    "\n",
    "# length of segments over which analysis is run:\n",
    "segment_length = 4\n",
    "conns = []\n",
    "ar_labels = []\n",
    "ar_badepochs = []\n",
    "for raw, event in tqdm(zip(raws(), events()), total=n):\n",
    "    print(raw.info['subject_info'])\n",
    "    original_frequency = raw.info['sfreq']\n",
    "\n",
    "    # load the raw data\n",
    "    raw = raw.load_data()\n",
    "    subid = (raw.info['subject_info']['pid'])\n",
    "    \n",
    "    # create a new virtual channel for the VEOG and HEOG\n",
    "    veog, _ = raw[mne.pick_channels(raw.ch_names, include=['EXG3', 'EXG4']), :]\n",
    "    veog = np.diff(veog, axis=0)\n",
    "    heog, _ = raw[mne.pick_channels(raw.ch_names, include=['EXG1', 'EXG2']), :]\n",
    "    heog = np.diff(heog, axis=0)\n",
    "    \n",
    "    new_info = mne.create_info(['VEOG', 'HEOG'], original_frequency, ch_types='eog')\n",
    "    raw.info['lowpass'] = new_info['lowpass'] = original_frequency\n",
    "    new_info['meas_date'] = raw.info['meas_date']\n",
    "    \n",
    "    # add these two new channels to the data\n",
    "    raw = raw.add_channels(\n",
    "        [mne.io.RawArray(np.concatenate([veog, heog], axis=0), new_info)]\n",
    "    )\n",
    "    \n",
    "    # drop extra channels (not used)\n",
    "    raw = raw.drop_channels(['EXG7', 'EXG8'])\n",
    "    # resample the data (w/ parallel processing)\n",
    "    raw = raw.resample(256, n_jobs = 6)\n",
    "\n",
    "\n",
    "    # set the average of the mastoids as reference\n",
    "    raw = raw.set_eeg_reference()\n",
    "    raw = raw.apply_proj()\n",
    "        \n",
    "    # filter the EOG data\n",
    "    raw = raw.filter(0.5, 15, picks=mne.pick_types(raw.info, eeg=False, eog=True), n_jobs=6, verbose=False)\n",
    "    # filter the EEG data (bandstop for mains and all harmonics)\n",
    "    raw = raw.notch_filter(np.arange(50, raw.info['lowpass'], 50), n_jobs=6, verbose=False)\n",
    "    # high-pass filter the data\n",
    "    raw = raw.filter(0.5, 45, n_jobs=6, verbose=False)\n",
    "        \n",
    "#     # Run ICA\n",
    "#     # -------  \n",
    "#     print('Subject ICA ID: {subid}'.format(subid=subid))\n",
    "    \n",
    "#     ica_file = 'data/interim/{subid}-ica.fif.gz'.format(subid=subid)\n",
    "#     # the following is only run if no ICA is present so far\n",
    "#     if not os.path.isfile(ica_file):\n",
    "#         # run an ICA to remove components correlated with both horizontal and vertical EOG traces\n",
    "#         seed = np.random.RandomState(2017)\n",
    "#         ica = mne.preprocessing.run_ica(raw, n_components=0.99, max_pca_components=64,\n",
    "#                                         random_state=seed, eog_ch=['HEOG', 'VEOG'],\n",
    "#                                         picks=mne.pick_types(raw.info, eeg=True))\n",
    "#         # save ICA to .fif file\n",
    "#         ica.save(ica_file)\n",
    "#     else:\n",
    "#         ica = mne.preprocessing.read_ica(ica_file)\n",
    "\n",
    "#     # plot the components to be excluded:\n",
    "#     ica.plot_components(picks=ica.exclude, title=subid, colorbar=True)\n",
    "#     # apply the ICA correction:\n",
    "#     raw = ica.apply(raw)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Epoch the data\n",
    "    # --------------\n",
    "    # edit the event structure to chop into 4s segments:\n",
    "    sample_frequency = raw.info['sfreq']\n",
    "    starttimes = event[(1, 3), 0] // (original_frequency / sample_frequency)\n",
    "    event = np.stack([\n",
    "        np.concatenate([np.arange(time + (segment_length / 2) * sample_frequency,\n",
    "                                  time + (60 - segment_length / 2) * sample_frequency,\n",
    "                                  segment_length * sample_frequency,\n",
    "                                  dtype=int)\n",
    "                        for time in starttimes]),\n",
    "        np.zeros((60 - segment_length) // 2, dtype=int),\n",
    "        102 + np.zeros((60 - segment_length) // 2, dtype=int)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # epoch the data to include only eyes closed:\n",
    "    epochs = mne.Epochs(raw, event, event_id={'eyes closed': 102}, tmin=0, tmax=segment_length, preload=True)\n",
    "    \n",
    "    \n",
    "    # autoreject bad epochs\n",
    "    # ---------------------\n",
    "    np.random.seed(42)\n",
    "    ar = AutoReject(n_jobs=6, verbose=False)\n",
    "    ar.fit(epochs)\n",
    "    epochs_clean, rl = ar.transform(epochs, return_log=True)\n",
    "\n",
    "#     # check data after  preprocessing\n",
    "#     epochs_clean.plot(n_channels=64, scalings={'eeg': 10e-5})\n",
    "#     plt.show()\n",
    "\n",
    "    print(\"{:.2f}% epochs rejected (N={})\".format(\n",
    "          epochs_clean.drop_log_stats(), len(epochs_clean)))\n",
    "    ar_labels.append(rl.labels)\n",
    "    ar_badepochs.append(rl.bad_epochs)\n",
    "\n",
    "    # Connectivity analysis\n",
    "    # ---------------------\n",
    "    connectivity_file = 'Output/{subid}_conn.csv'.format(subid=subid)\n",
    "    if not os.path.isfile(connectivity_file):\n",
    "        picks = mne.pick_types(epochs_clean.info, eeg=True)\n",
    "        connection_pairs = mne.connectivity.seed_target_indices(picks, picks)\n",
    "        \n",
    "        # which frequency bands to use:\n",
    "        freqbands = {'theta': (4, 8), 'alpha': (8, 14), 'beta': (14, 30), 'gamma': (30, 60)}\n",
    "        fmin = np.array([f for f, _ in freqbands.values()])\n",
    "        fmax = np.array([f for _, f in freqbands.values()])\n",
    "        \n",
    "        # perform the connectivity analysis:\n",
    "        conn, freqs, times, n_epochs, n_tapers = mne.connectivity.spectral_connectivity(\n",
    "            epochs_clean, method='wpli', mode='multitaper', fmin=fmin, fmax=fmax, faverage=True,\n",
    "            indices=connection_pairs, n_jobs=8, verbose=False\n",
    "        )\n",
    "        conns.append(conn)\n",
    "        \n",
    "        # save individual arrays of each subject (there is probably a better way than to go via numpy to pandas to save this...)\n",
    "        pd.DataFrame(\n",
    "            dict(channel_1=connection_pairs[0], channel_2=connection_pairs[1],\n",
    "                 **{key: conn[:, idx] for idx, key in enumerate(freqbands.keys())})\n",
    "        ).to_csv(connectivity_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC of bad epochs\n",
    "\n",
    "if os.path.isfile('Output/subjects.h5'):\n",
    "    subjects = pd.read_hdf('Output/subjects.h5', 'subjects')\n",
    "else:\n",
    "    subj = []\n",
    "    grps = []\n",
    "    bads = []\n",
    "    epch = []\n",
    "    for s_idx, raw in enumerate(raws()):\n",
    "        subj.append(raw.info['subject_info']['pid'])\n",
    "        grps.append(raw.info['subject_info']['group'])\n",
    "        bads.append(ar_badepochs[s_idx].sum())\n",
    "        epch.append(len(ar_badepochs[s_idx]))\n",
    "\n",
    "    subjects = pd.DataFrame(np.column_stack([subj, grps, bads, epch]),\n",
    "                            columns=['subject', 'group', 'bad_epochs', 'tot_epochs'])\n",
    "\n",
    "    subjects.bad_epochs = subjects.bad_epochs.astype(int)\n",
    "    subjects.tot_epochs = subjects.tot_epochs.astype(int) \n",
    "    df.to_hdf('Output/subjects.h5', key='subjects', mode='w')\n",
    "\n",
    "sns.set(style=\"whitegrid\",font_scale=2)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(12, 11))\n",
    "dy = \"bad_epochs\"; dx = \"group\"; ort = \"v\"\n",
    "# Draw a violinplot with a narrower bandwidth than the default\n",
    "ax=pt.half_violinplot(data = subjects, palette = \"Set2\", bw=.2,  linewidth=1,cut=0.,\\\n",
    "                   scale=\"area\", width=.8, inner=None,orient=ort,x=dx,y=dy)\n",
    "ax=sns.stripplot(data = subjects, palette=\"Set2\", edgecolor=\"white\",size=10,orient=ort,\\\n",
    "                 x=dx,y=dy,jitter=1,zorder=0)\n",
    "ax=sns.boxplot(data = subjects, color=\"black\",orient=ort,width=.15,x=dx,y=dy,zorder=10,\\\n",
    "              showcaps=True,boxprops={'facecolor':'none', \"zorder\":10},\\\n",
    "               showfliers=True,whiskerprops={'linewidth':2, \"zorder\":10},saturation=1)\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor mapping for connectivity measures & data saving\n",
    "if os.path.isfile('Output/processed_results.h5'):\n",
    "    hf = h5py.File('Output/processed_results.h5', 'r')\n",
    "    vectconn = hf['vectconn']\n",
    "    frequencies_labels = [l.astype(str) for l in hf['frequencies_labels']]\n",
    "    electrodes = hf['electrodes']\n",
    "    idx = hf['idx']\n",
    "    ar_badepochs = hf['ar_badepochs']\n",
    "else:\n",
    "    A = mne.channels.find_ch_connectivity(raw.info, ch_type='eeg')\n",
    "    bin=0\n",
    "    idx = []\n",
    "    electrodes = []\n",
    "    for e1 in range(64):\n",
    "        for e2 in range(64):\n",
    "            if e2>e1:\n",
    "                idx.append(bin)\n",
    "                electrodes.append((e1, e2))\n",
    "            bin = bin + 1\n",
    "\n",
    "    frequencies_labels = ['theta', 'alpha', 'beta', 'gamma']\n",
    "    vectconn = np.zeros([len(conns), len(frequencies_labels), len(idx)])\n",
    "    for f_idx, f_lab in enumerate(frequencies_labels):\n",
    "        for s_idx, s_conn in enumerate(conns):\n",
    "            vectconn[s_idx, f_idx, :] = s_conn[idx, f_idx]\n",
    "    hf = h5py.File('Output/processed_results.h5', 'w')\n",
    "    hf.create_dataset('vectconn', data=vectconn)\n",
    "    hf.create_dataset('frequencies_labels', data=np.array(frequencies_labels, dtype='S'))\n",
    "    hf.create_dataset('electrodes', data=electrodes)\n",
    "    hf.create_dataset('idx', data=idx)\n",
    "    hf.create_dataset('ar_badepochs', data=ar_badepochs)\n",
    "    hf.create_dataset('subjects', data=subjects)    \n",
    "    hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connectivity measures Joyplots\n",
    "\n",
    "subj = []\n",
    "grps = []\n",
    "freq = []\n",
    "vals = []\n",
    "for f_idx, f_lab in enumerate(frequencies_labels):\n",
    "    for s_idx, raw in enumerate(raws()):\n",
    "        subj.append([raw.info['subject_info']['pid']]*2016)\n",
    "        grps.append([raw.info['subject_info']['group']]*2016)\n",
    "        freq.append([f_lab]*2016)\n",
    "        vals.append(vectconn[s_idx, f_idx])\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "        \n",
    "subj = flatten(subj)\n",
    "grps = flatten(grps)\n",
    "freq = flatten(freq)\n",
    "vals = flatten(vals)\n",
    "\n",
    "df = pd.DataFrame(np.column_stack([subj, grps, freq, vals]),\n",
    "                  columns=['subject', 'group', 'frequency', 'wPLI'])\n",
    "\n",
    "df.wPLI = df.wPLI.astype(float) \n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "joypy.joyplot(df.loc[df.group=='CTR'], column=\"wPLI\", by=[\"subject\"],\n",
    "              fill=False, background='k', linecolor=\"w\",\n",
    "              legend=False, overlap=0.5, figsize=(6,5),kind=\"counts\",\n",
    "              x_range=[-0.1,1.1], bins=80)\n",
    "plt.figure(figsize=(8,4))\n",
    "joypy.joyplot(df.loc[df.group=='ASC'], column=\"wPLI\", by=[\"subject\"],\n",
    "              fill=False, background='k', linecolor=\"w\",\n",
    "              legend=False, overlap=0.5, figsize=(6,5),kind=\"counts\",\n",
    "              x_range=[-0.1,1.1], bins=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metaconnectivity for cluster permutation of bivariate measurements\n",
    "\n",
    "if os.path.isfile('Output/metaconn.h5'):\n",
    "    hf = h5py.File('Output/metaconn.h5', 'r')\n",
    "    metaconn = hf['metaconn']\n",
    "else:\n",
    "    metaconn = np.zeros((vectconn.shape[-1], vectconn.shape[-1]))\n",
    "\n",
    "    for ne1, (e11,e12) in tqdm(enumerate(electrodes)):\n",
    "        for ne2, (e21,e22) in enumerate(electrodes):\n",
    "            # print(ne1,e11,e12,ne2,e21,e22)\n",
    "            metaconn[ne1, ne2] = (((A[0][e11,e21]) and (A[0][e12,e22])) or \n",
    "                                  ((A[0][e11,e22]) and (A[0][e12,e21])) or\n",
    "                                  ((A[0][e11,e21]) and (e12 == e22)) or \n",
    "                                  ((A[0][e12,e21]) and (e11 == e22)) or\n",
    "                                  ((A[0][e11,e22]) and (e12 == e21)) or \n",
    "                                  ((A[0][e12,e22]) and (e11 == e21)))\n",
    "\n",
    "    hf = h5py.File('Output/metaconn.h5', 'w')\n",
    "    hf.create_dataset('metaconn', data=metaconn)\n",
    "    hf.close()\n",
    "\n",
    "plt.figure(figsize=[10,8])\n",
    "plt.spy(metaconn)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sensors neighborood\n",
    "sensloc = np.array([c['loc'][:3] for c in raw.info['chs']][:64])\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.scatter(sensloc[:, 0], sensloc[:, 1])\n",
    "for e1 in range(64):\n",
    "    for e2 in range(64):\n",
    "        if A[0][e1,e2]:\n",
    "            plt.plot((sensloc[e1, 0], sensloc[e2, 0]), (sensloc[e1, 1], sensloc[e2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyses & visualization of connectivity modulation clusters\n",
    "\n",
    "grps = np.zeros(vectconn.shape[0])\n",
    "for s_idx, raw in enumerate(raws()):\n",
    "    if raw.info['subject_info']['group']=='ASC':\n",
    "        grps[s_idx] = 1\n",
    "        \n",
    "plt.figure(figsize=[20,5])\n",
    "for f_idx, f_lab in enumerate(frequencies_labels):\n",
    "    print(f_lab)\n",
    "    data1 = vectconn[np.argwhere(grps==1), f_idx, :]\n",
    "    data2 = vectconn[np.argwhere(grps==0), f_idx, :]\n",
    "    Fobs, clusters, cluster_pv, H0 = mne.stats.permutation_cluster_test(X=[data1, data2], n_permutations=1024, connectivity=lil_matrix(metaconn))\n",
    "    plt.subplot(1,4,f_idx+1)\n",
    "    plt.scatter(sensloc[:, 0], sensloc[:, 1])\n",
    "    if len(cluster_pv)>0:\n",
    "        print(cluster_pv)\n",
    "        for cl_idx, cluster in enumerate(clusters):  # [np.argmin(cluster_pv)]\n",
    "            #if cluster_pv[cl_idx]<0.05:\n",
    "            color = np.random.rand(3)\n",
    "            for ne, (e1,e2) in enumerate(electrodes):\n",
    "                if cluster[ne]:\n",
    "                    print(cl_idx, e1, e2, Fobs[ne])\n",
    "                    plt.plot((sensloc[e1, 0], sensloc[e2, 0]), (sensloc[e1, 1], sensloc[e2, 1]), linewidth=5, color=color)\n",
    "            plt.title(f_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph theory sandbox\n",
    "import networkx as nx\n",
    "# Convert flatten connectivity matrix into square shape\n",
    "# Also compute graph measures on the fly\n",
    "\n",
    "result = np.zeros((vectconn.shape[0], vectconn.shape[1], 64, 64))\n",
    "for f_idx, f_lab in enumerate(frequencies_labels):\n",
    "    for s_idx, row in enumerate(subjects.iterrows()):\n",
    "        tmp = np.zeros((64, 64))\n",
    "        ind = np.triu_indices(64,1)\n",
    "        tmp[ind] = vectconn[s_idx,f_idx,:]\n",
    "        tmp = tmp + tmp.T\n",
    "        result[s_idx, f_idx, :, :] = tmp\n",
    "        \n",
    "        G = nx.from_numpy_array(tmp)\n",
    "        T = nx.minimum_spanning_tree(G, weight = 'weight')\n",
    "\n",
    "        threshold = np.sort(tmp.flatten())[int(len(tmp.flatten())*0.9)]\n",
    "        for e in G.edges(data=True):\n",
    "            if e[2]['weight']>threshold:\n",
    "                T.add_edge(e[0], e[1], weight=e[2]['weight'])\n",
    "        \n",
    "        subjects.loc[row[0], 'assortativity_'+f_lab] = nx.degree_pearson_correlation_coefficient(T)\n",
    "        subjects.loc[row[0], 'clustering_'+f_lab] = nx.average_clustering(T)\n",
    "        subjects.loc[row[0], 'avg_shortest_path_'+f_lab] = nx.average_shortest_path_length(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[40, 30])\n",
    "bin=1\n",
    "for m_idx, m_lab in enumerate(['assortativity', 'clustering', 'avg_shortest_path']):\n",
    "    for f_idx, f_lab in enumerate(frequencies_labels):\n",
    "        plt.subplot(3, 4, bin)\n",
    "        dy = m_lab + \"_\" + f_lab; dx = \"group\"; ort = \"v\"\n",
    "        pt.half_violinplot(data = subjects, palette = \"Set\" + str(m_idx+1), bw=.2,  linewidth=1,cut=0.,\\\n",
    "                           scale=\"area\", width=.8, inner=None,orient=ort,x=dx,y=dy)\n",
    "        sns.stripplot(data = subjects, palette=\"Set\" + str(m_idx+1), edgecolor=\"white\",size=10,orient=ort,\\\n",
    "                         x=dx,y=dy,jitter=1,zorder=0)\n",
    "        sns.boxplot(data = subjects, color=\"black\",orient=ort,width=.15,x=dx,y=dy,zorder=10,\\\n",
    "                      showcaps=True,boxprops={'facecolor':'none', \"zorder\":10},\\\n",
    "                       showfliers=True,whiskerprops={'linewidth':2, \"zorder\":10},saturation=1)\n",
    "        sns.despine(left=True)\n",
    "        bin = bin+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
